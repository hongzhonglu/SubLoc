{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_augmentation_GAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLbZ29HyuQqv",
        "colab_type": "text"
      },
      "source": [
        "# Instructions on Google colab:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRi84eDsib0e",
        "colab_type": "text"
      },
      "source": [
        "Please follow the steps below, before running the script:\n",
        "\n",
        "\n",
        "1.  Navigate to `Runtime`>`Change runtime type` and select **GPU** as Hardware accelerator \n",
        "2.   Download [this](https://github.com/mgetech/SubLoc/blob/master/misc/data_augmentation_GAN.zip) as .zip \n",
        "1.   Choose `Files` from the side menu\n",
        "2.   Upload the downloaded .zip file\n",
        "3.   Run the following cells in order\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLGSJSmzuHsF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygrhAVOWy_1h",
        "colab_type": "text"
      },
      "source": [
        "The following code, which is inspired by [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](http://arxiv.org/abs/1609.05473), performs the **Data Augmentation** task. That is, if the amount of the annotated data in a certain class is limited, we use this method to generate synthetic data for that class. Therefore, the Machine Learning algorithm can train a model with adequate amount of data, which will result in higher discriminatory power."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfJik6Sqn1d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip data_augmentation_GAN.zip\n",
        "!pip install biopython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANLkficb3r10",
        "colab_type": "text"
      },
      "source": [
        "For example, the annotated data in *Cytoplasmic/Cytoplasmic Membrane* class is limited. Thus, the relevant FASTA file will prepared to be fed into the Data Augmentation script (SeqGAN) as the `positive_file`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iejDKcyIcrbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessing as prp\n",
        "reduced_X = prp.import_FastaSeq(\"save/cytoplasmiccytoplasmicmembrane-3_0.txt\")\n",
        "\n",
        "# Create input for SeqGAN\n",
        "prp.GAN_input(\"save/cytoplasmiccytoplasmicmembrane-3_0_GAN_input.txt\", reduced_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZwMOr-05D7e",
        "colab_type": "text"
      },
      "source": [
        "The SeqGAN also needs data from another class in the dataset to be able to generate synthetic data that can be discriminated from other classes, which will be prepared in the following cell and fed into the Data Augmentation script (SeqGAN) as the `negative_file`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5ft0tt4ilfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduced_X = prp.import_FastaSeq(\"save/cytoplasmicmembrane-3_0.txt\")\n",
        "\n",
        "# Create input for SeqGAN\n",
        "prp.GAN_input(\"save/cytoplasmicmembrane-3_0_GAN_input.txt\", reduced_X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-ksLsLf6UuO",
        "colab_type": "text"
      },
      "source": [
        "The address of the created files in the above cells, **must** be set in the `configuration.py` as `positive_file` and `negative_file` variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjdi8rTE7r4p",
        "colab_type": "text"
      },
      "source": [
        "Running the cells in the below, will generate synthetic data which is similar to the class with limited annotated data (positive_file) and will be saved in the address that has been set in the `configuration.py` as `negative_feedback` variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1h9m9QSkTVQ",
        "colab_type": "text"
      },
      "source": [
        "# SeqGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlX-PN8-suJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjAhh_T8kXOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import os\n",
        "#import cPickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.version)\n",
        "from configuration import *\n",
        "from utils import *\n",
        "from utils_fb import my_gen\n",
        "from feedback import pre\n",
        "from dataloader import Gen_Data_loader, Dis_dataloader\n",
        "from discriminator import Discriminator\n",
        "from generator import Generator\n",
        "from rollout import rollout\n",
        "#from target_lstm import TARGET_LSTM\n",
        "\n",
        "#Hardware related setting\n",
        "config_hardware = tf.ConfigProto()\n",
        "config_hardware.gpu_options.per_process_gpu_memory_fraction = 0.40\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "def main(unused_argv):\n",
        "    config_train = training_config()\n",
        "    config_gen = generator_config()\n",
        "    config_dis = discriminator_config()\n",
        "    np.random.seed(config_train.seed)\n",
        "    assert config_train.start_token == 0\n",
        "\n",
        "    #Build dataloader for generaotr, testing and discriminator\n",
        "    gen_data_loader = Gen_Data_loader(config_gen.gen_batch_size)\n",
        "    likelihood_data_loader = Gen_Data_loader(config_gen.gen_batch_size)\n",
        "    dis_data_loader = Dis_dataloader(config_dis.dis_batch_size)\n",
        "\n",
        "    #Build generator and its rollout\n",
        "    generator = Generator(config=config_gen)\n",
        "    generator.build()\n",
        "    rollout_gen = rollout(config=config_gen)\n",
        "\n",
        "    #Build target LSTM\n",
        "    #target_params = cPickle.load(open('save/target_params.pkl'))\n",
        "    #print(target_params)\n",
        "    #target_lstm = TARGET_LSTM(config=config_gen, params=target_params) # The oracle model\n",
        "\n",
        "    #Build discriminator\n",
        "    discriminator = Discriminator(config=config_dis)\n",
        "    discriminator.build_discriminator()\n",
        "\n",
        "    #Build optimizer op for pretraining\n",
        "    pretrained_optimizer = tf.train.AdamOptimizer(config_train.gen_learning_rate)\n",
        "    var_pretrained = [v for v in tf.trainable_variables() if 'teller' in v.name] #Using name 'teller' here to prevent name collision of target LSTM\n",
        "    gradients, variables = zip(*pretrained_optimizer.compute_gradients(generator.pretrained_loss, var_list=var_pretrained))\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, config_train.grad_clip)\n",
        "    gen_pre_upate = pretrained_optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    #Initialize all variables\n",
        "    sess = tf.Session(config=config_hardware)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    #Initalize data loader of generator\n",
        "    #generate_samples(sess, target_lstm, config_train.batch_size, config_train.generated_num, config_train.positive_file)\n",
        "    gen_data_loader.create_batches(config_train.positive_file)\n",
        "    '''\n",
        "    #Start pretraining\n",
        "    log = open('save/experiment-log.txt', 'w')\n",
        "    print 'Start pre-training generator...'\n",
        "    log.write('pre-training...\\n')\n",
        "    for epoch in range(config_train.pretrained_epoch_num):\n",
        "        gen_data_loader.reset_pointer()\n",
        "        for it in range(gen_data_loader.num_batch):\n",
        "            batch = gen_data_loader.next_batch()\n",
        "            _, g_loss = sess.run([gen_pre_upate, generator.pretrained_loss], feed_dict={generator.input_seqs_pre:batch,\\\n",
        "                                                                                       generator.input_seqs_mask:np.ones_like(batch)})\n",
        "        \n",
        "        if epoch % config_train.test_per_epoch == 0:\n",
        "            #generate_samples(sess, generator, config_train.batch_size, config_train.generated_num, config_train.eval_file)\n",
        "            likelihood_data_loader.create_batches(config_train.eval_file)\n",
        "            #test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
        "            #print 'pre-train epoch ', epoch, 'test_loss ', test_loss\n",
        "            print 'pre-train epoch ', epoch\n",
        "            #buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
        "            buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' \n",
        "            log.write(buffer)\n",
        "        \n",
        "    '''\n",
        "    print ('Start pre-training discriminator...')\n",
        "    for t in range(config_train.dis_update_time_pre):\n",
        "        print (\"Times: \" + str(t))\n",
        "        #generate_samples(sess, generator, config_train.batch_size, config_train.generated_num, config_train.negative_file)\n",
        "        dis_data_loader.load_train_data(config_train.positive_file, config_train.negative_file)\n",
        "        for _ in range(config_train.dis_update_epoch_pre):\n",
        "            dis_data_loader.reset_pointer()\n",
        "            for it in range(dis_data_loader.num_batch):\n",
        "                x_batch, y_batch = dis_data_loader.next_batch()\n",
        "                feed = {\n",
        "                    discriminator.input_x: x_batch,\n",
        "                    discriminator.input_y: y_batch,\n",
        "                    discriminator.dropout_keep_prob: config_dis.dis_dropout_keep_prob\n",
        "                }\n",
        "                _ = sess.run(discriminator.train_op, feed)\n",
        "    \n",
        "    #Build optimizer op for adversarial training\n",
        "    train_adv_opt = tf.train.AdamOptimizer(config_train.gen_learning_rate)\n",
        "    gradients, variables = zip(*train_adv_opt.compute_gradients(generator.gen_loss_adv,var_list=var_pretrained))\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, config_train.grad_clip)\n",
        "    train_adv_update = train_adv_opt.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    #Initialize global variables of optimizer for adversarial training\n",
        "    uninitialized_var = [e for e in tf.global_variables() if e not in tf.trainable_variables()]\n",
        "    init_vars_uninit_op = tf.variables_initializer(uninitialized_var)\n",
        "    sess.run(init_vars_uninit_op)\n",
        "    \n",
        "    #Start adversarial training\n",
        "    print ('Start adversarial training')\n",
        "    for total_batch in range(config_train.total_batch):\n",
        "        print(f\"total_batch: {total_batch}\")\n",
        "\n",
        "\n",
        "        for iter_gen in range(config_train.gen_update_time):\n",
        "            samples = sess.run(generator.sample_word_list_reshape)\n",
        "            #samples = pre(samples,0.8)\n",
        "            #print(len(samples))\n",
        "            feed = {\"pred_seq_rollout:0\":samples}\n",
        "            reward_rollout = []\n",
        "            #calcuate the reward given in the specific stpe t by roll out\n",
        "            for iter_roll in range(config_train.rollout_num):\n",
        "                rollout_list = sess.run(rollout_gen.sample_rollout_step, feed_dict=feed)\n",
        "                rollout_list_stack = np.vstack(rollout_list) #shape: #batch_size * #rollout_step, #sequence length\n",
        "                reward_rollout_seq = sess.run(discriminator.ypred_for_auc, feed_dict={discriminator.input_x:rollout_list_stack, discriminator.dropout_keep_prob:1.0})\n",
        "                reward_last_tok = sess.run(discriminator.ypred_for_auc, feed_dict={discriminator.input_x:samples, discriminator.dropout_keep_prob:1.0})\n",
        "                reward_allseq = np.concatenate((reward_rollout_seq, reward_last_tok), axis=0)[:,1]\n",
        "                reward_tmp = []\n",
        "                for r in range(config_gen.gen_batch_size):\n",
        "                    reward_tmp.append(reward_allseq[range(r, config_gen.gen_batch_size * config_gen.sequence_length, config_gen.gen_batch_size)])\n",
        "                reward_rollout.append(np.array(reward_tmp))\n",
        "            rewards = np.sum(reward_rollout, axis=0)/config_train.rollout_num\n",
        "            _, gen_loss = sess.run([train_adv_update, generator.gen_loss_adv], feed_dict={generator.input_seqs_adv:samples,\\\n",
        "                                                                                        generator.rewards:rewards})\n",
        "\n",
        "        for _ in range(config_train.dis_update_time_adv):\n",
        "            my_gen(sess, generator, config_train.batch_size, config_train.generated_num, config_train.negative_feedback)\n",
        "            generate_samples(sess, generator, config_train.batch_size, config_train.generated_num, config_train.negative_file)\n",
        "            dis_data_loader.load_train_data(config_train.negative_feedback, config_train.negative_file)\n",
        "            for _ in range(config_train.dis_update_epoch_adv):\n",
        "                dis_data_loader.reset_pointer()\n",
        "                for it in range(dis_data_loader.num_batch):\n",
        "                    x_batch, y_batch = dis_data_loader.next_batch()\n",
        "                    feed = {\n",
        "                        discriminator.input_x: x_batch,\n",
        "                        discriminator.input_y: y_batch,\n",
        "                        discriminator.dropout_keep_prob: config_dis.dis_dropout_keep_prob\n",
        "                    }\n",
        "                    _ = sess.run(discriminator.train_op, feed)\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmX8mxgl9lKW",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion:** Combining the limited annotated data with the generated synthetic data will augment the data for this class, which will result in more **accurate** and **precise** **predictions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_dYibFzKVWA",
        "colab_type": "text"
      },
      "source": [
        "Note: This code is based on the previous work by [LantaoYu](https://github.com/LantaoYu/SeqGAN)  and [1073521013](https://github.com/1073521013/PSL-DL/tree/master/PSL_GAN). Many thanks to [LantaoYu](https://github.com/LantaoYu)  and [1073521013](https://github.com/1073521013/)."
      ]
    }
  ]
}